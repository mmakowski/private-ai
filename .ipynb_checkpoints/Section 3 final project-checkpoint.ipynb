{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions\n",
    "\n",
    "* like project 2, but the aggregator does not see the individual model updates -- they are encrypted for additive secret sharing\n",
    "\n",
    "## Set up workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.13.1). Fix this by compiling custom ops.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import syft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = syft.TorchHook(torch)\n",
    "\n",
    "num_workers = 10\n",
    "workers = [syft.VirtualWorker(hook, id=i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<VirtualWorker id:0 #objects:0>,\n",
       " <VirtualWorker id:1 #objects:0>,\n",
       " <VirtualWorker id:2 #objects:0>,\n",
       " <VirtualWorker id:3 #objects:0>,\n",
       " <VirtualWorker id:4 #objects:0>,\n",
       " <VirtualWorker id:5 #objects:0>,\n",
       " <VirtualWorker id:6 #objects:0>,\n",
       " <VirtualWorker id:7 #objects:0>,\n",
       " <VirtualWorker id:8 #objects:0>,\n",
       " <VirtualWorker id:9 #objects:0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All workers will be differentiators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "differentiators = workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "Prepare training data and distribute among the differentiators. We will try to learn a 10-dimensional linear model. To make things more interesting, only one of the workers will have the data corresponding to one of the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 10\n",
    "true_coefficients = torch.tensor(range(1,model_dim+2)).float() # includes bias term\n",
    "num_examples_per_differentiator = 100\n",
    "\n",
    "X_ptrs = []\n",
    "y_ptrs = []\n",
    "\n",
    "for i in range(len(differentiators)):\n",
    "    X = torch.cat((torch.rand((num_examples_per_differentiator, model_dim)),\n",
    "                   torch.tensor([1.0] * num_examples_per_differentiator).view((-1, 1))), # additional dimension for bias\n",
    "                  dim=1)\n",
    "    # only differentiator 3 knows about the 3rd parameter\n",
    "    if i != 3:\n",
    "        X[:, 2] = 0\n",
    "    y = (torch.matmul(X, true_coefficients)).view((num_examples_per_differentiator, 1))\n",
    "    X_ptrs.append(X.send(differentiators[i]))\n",
    "    y_ptrs.append(y.send(differentiators[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_model(template=None):\n",
    "    if template is not None:\n",
    "        return template.clone().detach().requires_grad_(True)\n",
    "    else:\n",
    "        return torch.tensor(()).new_zeros((model_dim+1, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "We will share the updated model with all other workers and aggregate in the shared tensor, then collect it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.7460, 3.7535, 0.3932, 3.7660, 3.7440, 3.6029, 3.9330, 3.7573, 3.7824,\n",
      "         3.9096, 7.3983]])\n",
      "tensor([[ 1.9270,  2.5653,  1.7514,  4.1688,  5.2090,  5.8132,  6.8284,  7.4948,\n",
      "          8.4798,  9.1060, 11.2574]])\n",
      "tensor([[ 1.1647,  2.0517,  2.3154,  3.9869,  5.0231,  5.9207,  6.9493,  7.8678,\n",
      "          8.8931,  9.7841, 11.2116]])\n",
      "tensor([[ 1.0128,  1.9750,  2.6016,  3.9747,  4.9799,  5.9544,  6.9652,  7.9495,\n",
      "          8.9576,  9.9332, 11.1672]])\n",
      "tensor([[ 0.9896,  1.9703,  2.7486,  3.9739,  4.9776,  5.9615,  6.9664,  7.9695,\n",
      "          8.9679,  9.9685, 11.1381]])\n",
      "tensor([[ 0.9890,  1.9709,  2.8449,  3.9734,  4.9771,  5.9662,  6.9710,  7.9720,\n",
      "          8.9720,  9.9765, 11.1215]])\n",
      "tensor([[ 0.9889,  1.9728,  2.8570,  3.9733,  4.9780,  5.9681,  6.9720,  7.9730,\n",
      "          8.9729,  9.9774, 11.1163]])\n",
      "tensor([[ 0.9889,  1.9728,  2.8570,  3.9733,  4.9780,  5.9681,  6.9720,  7.9730,\n",
      "          8.9729,  9.9774, 11.1163]])\n",
      "tensor([[ 0.9889,  1.9728,  2.8570,  3.9733,  4.9780,  5.9681,  6.9720,  7.9730,\n",
      "          8.9729,  9.9774, 11.1163]])\n",
      "tensor([[ 0.9889,  1.9728,  2.8570,  3.9733,  4.9780,  5.9681,  6.9720,  7.9730,\n",
      "          8.9729,  9.9774, 11.1163]])\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = mk_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    agg_model_shared = mk_model().requires_grad_(False).fix_prec().share(*differentiators)\n",
    "    \n",
    "    for i in range(len(differentiators)):\n",
    "        worker = differentiators[i]\n",
    "        model_ptr = mk_model(model).send(worker)\n",
    "        pred_ptr = X_ptrs[i].mm(model_ptr)\n",
    "        loss_ptr = ((pred_ptr - y_ptrs[i])**2).sum()\n",
    "        loss_ptr.backward()\n",
    "        model_ptr.data.sub_(model_ptr.grad * learning_rate)\n",
    "        model_shared = model_ptr.fix_prec().share(*differentiators).get()  # ensure both model and agg model pointers are shared from me\n",
    "        agg_model_shared += model_shared\n",
    "\n",
    "    model = agg_model_shared.get().float_prec() / len(differentiators)\n",
    "    if epoch % 100 == 0:\n",
    "        print(model.view(1, -1).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions\n",
    "\n",
    "* the gradients are not seen \"locally\" (are only visible to the workers)\n",
    "* all gradients are collected on one of the workers and \"aggregated\" (averaged?) there\n",
    "* then the averaged gradients can be brought locally\n",
    "* use `move` to collect all gradients on one of the workers\n",
    "\n",
    "## Set up workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.13.1). Fix this by compiling custom ops.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import syft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = syft.TorchHook(torch)\n",
    "\n",
    "num_workers = 10\n",
    "workers = [syft.VirtualWorker(hook, id=i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<VirtualWorker id:0 #tensors:0>,\n",
       " <VirtualWorker id:1 #tensors:0>,\n",
       " <VirtualWorker id:2 #tensors:0>,\n",
       " <VirtualWorker id:3 #tensors:0>,\n",
       " <VirtualWorker id:4 #tensors:0>,\n",
       " <VirtualWorker id:5 #tensors:0>,\n",
       " <VirtualWorker id:6 #tensors:0>,\n",
       " <VirtualWorker id:7 #tensors:0>,\n",
       " <VirtualWorker id:8 #tensors:0>,\n",
       " <VirtualWorker id:9 #tensors:0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a dedicated worker that does the aggregation and will use the remaining ones for calculating gradients based on individual data items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = workers[0]\n",
    "differentiators = workers[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "Prepare training data and distribute among the differentiators. We will try to learn a 10-dimensional linear model. To make things more interesting, only one of the workers will have the data corresponding to one of the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 10\n",
    "true_coefficients = torch.tensor(range(1,model_dim+2)).float() # includes bias term\n",
    "num_examples_per_differentiator = 100\n",
    "\n",
    "X_ptrs = []\n",
    "y_ptrs = []\n",
    "\n",
    "for i in range(len(differentiators)):\n",
    "    X = torch.cat((torch.rand((num_examples_per_differentiator, model_dim)),\n",
    "                   torch.tensor([1.0] * num_examples_per_differentiator).view((-1, 1))), # additional dimension for bias\n",
    "                  dim=1)\n",
    "    # only differentiator 3 knows about the 3rd parameter\n",
    "    if i != 3:\n",
    "        X[:, 2] = 0\n",
    "    y = (torch.matmul(X, true_coefficients)).view((num_examples_per_differentiator, 1))\n",
    "    X_ptrs.append(X.send(differentiators[i]))\n",
    "    y_ptrs.append(y.send(differentiators[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_model(template=None):\n",
    "    if template is not None:\n",
    "        return template.clone().detach().requires_grad_(True)\n",
    "    else:\n",
    "        return torch.tensor(()).new_zeros((model_dim+1, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The instructions said to use move to get the gradients to a single worker. However, [there is a bug](https://openmined.slack.com/archives/C6DEWA4FR/p1562282512181200?thread_ts=1562282454.181100&cid=C6DEWA4FR) where the gradients don't get moved alongside the tensor. We will therefore apply the gradients on each differentiator to a copy of the model, the collect and average all copies of the model on the aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.7783, 3.7443, 0.4085, 3.7935, 3.7636, 3.7254, 3.9816, 3.7842, 3.8019,\n",
      "         3.8849, 7.4163]])\n",
      "tensor([[ 1.8421,  2.6393,  1.7548,  4.2687,  5.2593,  6.0991,  6.6403,  7.5424,\n",
      "          8.4513,  9.2471, 11.0668]])\n",
      "tensor([[ 1.1512,  2.1121,  2.3329,  4.0354,  5.0716,  6.0538,  6.8793,  7.8972,\n",
      "          8.9156,  9.8638, 11.0466]])\n",
      "tensor([[ 1.0252,  2.0182,  2.6399,  3.9995,  5.0158,  6.0160,  6.9548,  7.9713,\n",
      "          8.9901,  9.9755, 11.0371]])\n",
      "tensor([[ 1.0019,  2.0011,  2.8050,  3.9952,  5.0015,  6.0027,  6.9797,  7.9889,\n",
      "          8.9999,  9.9952, 11.0280]])\n",
      "tensor([[ 0.9981,  1.9982,  2.8942,  3.9958,  4.9984,  5.9992,  6.9894,  7.9942,\n",
      "          9.0003,  9.9985, 11.0201]])\n",
      "tensor([[ 0.9979,  1.9981,  2.9425,  3.9969,  4.9982,  5.9987,  6.9939,  7.9964,\n",
      "          8.9998,  9.9990, 11.0138]])\n",
      "tensor([[ 0.9984,  1.9985,  2.9687,  3.9979,  4.9986,  5.9989,  6.9964,  7.9977,\n",
      "          8.9996,  9.9992, 11.0093]])\n",
      "tensor([[ 0.9989,  1.9989,  2.9829,  3.9986,  4.9990,  5.9992,  6.9978,  7.9985,\n",
      "          8.9996,  9.9994, 11.0061]])\n",
      "tensor([[ 0.9992,  1.9993,  2.9906,  3.9991,  4.9993,  5.9995,  6.9987,  7.9990,\n",
      "          8.9997,  9.9996, 11.0040]])\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = mk_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    agg_model_ptr = mk_model().requires_grad_(False).send(aggregator)\n",
    "    \n",
    "    for i in range(len(differentiators)):\n",
    "        worker = differentiators[i]\n",
    "        model_ptr = mk_model(model).send(worker)\n",
    "        pred_ptr = X_ptrs[i].mm(model_ptr)\n",
    "        loss_ptr = ((pred_ptr - y_ptrs[i])**2).sum()\n",
    "        loss_ptr.backward()\n",
    "        model_ptr.data.sub_(model_ptr.grad * learning_rate)\n",
    "        model_ptr.move(aggregator)\n",
    "        agg_model_ptr += model_ptr\n",
    "\n",
    "    model = agg_model_ptr.get() / len(differentiators)\n",
    "    if epoch % 100 == 0:\n",
    "        print(model.view(1, -1).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the 3rd dimension converges more slowly than the others, due to only one worker having the data for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
